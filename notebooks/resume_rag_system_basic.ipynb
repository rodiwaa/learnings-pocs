{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4/ptuzQREcCyB/GKIfDBn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rodiwaa/learnings-pocs/blob/main/notebooks/resume_rag_system_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kkC9CEacOPOU"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain.openai openai langchain-community langsmith chromadb python-dotenv sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## API keys from .env"
      ],
      "metadata": {
        "id": "ym20FDXoPOo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "load_dotenv(dotenv_path=\"/content/drive/MyDrive/Projects/.env/.env\")"
      ],
      "metadata": {
        "id": "ixIr6bmoPTmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read pdf from drive\n",
        "HOLD. will work with docs now, import PDF later."
      ],
      "metadata": {
        "id": "HcPXAkBARH4J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create docs for \"about me\""
      ],
      "metadata": {
        "id": "zax1ZAyOSHP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.vectorstores import Chroma\n",
        "# from langchain_openai import OpenAIEmbeddings\n",
        "# from sentence_transformers import SentenceTransformer # does not work well w langchain/chroma; use SentenceTransformerEmbeddings instead\n",
        "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
        "\n",
        "docs = [\n",
        "    Document(page_content=\"Jack likes to build workflows and AI systems\"),\n",
        "    Document(page_content=\"Jack has worked on following technology stacks - Langchain, Langgraph, Langsmith\"),\n",
        "    Document(page_content=\"Jack is friends with Tom and Sally.\"),\n",
        "    Document(page_content=\"Jack loves to trek on weekends.\"),\n",
        "    Document(page_content=\"Jack loves to watch movies and listen to music.\")\n",
        "]\n",
        "\n",
        "embedding_model = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "    embedding = embedding_model,\n",
        "    documents = docs,\n",
        "    collection_name = \"random_db_2\",\n",
        "    persist_directory = \"random_db_2\"\n",
        ")\n",
        "\n",
        "# check if docs are added\n",
        "added_docs = vectorstore.get()\n",
        "\n",
        "print(f\"added {len(added_docs)} docs\")\n",
        "print(added_docs)"
      ],
      "metadata": {
        "id": "3azxnay7RMQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrieve docs from VS based on query sim searches"
      ],
      "metadata": {
        "id": "zm_I96doji3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# delete chromadb to start from scratch\n",
        "# vectorstore.delete_collection()\n",
        "\n",
        "added = vectorstore.get()\n",
        "added"
      ],
      "metadata": {
        "id": "W8a_hI2OHyEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ignore this cell; i've broken this into indi cells below\n",
        "\n",
        "# vector store sim search\n",
        "QUERY1 = \"who are Jack's friends?\"\n",
        "QUERY2 = \"what are jack's hobbies?\"\n",
        "QUERY3 = \"what does jack work on?\"\n",
        "\n",
        "# basic sim search\n",
        "basic_search = vectorstore.similarity_search(\n",
        "    query = QUERY2,\n",
        "    k = 2\n",
        ")\n",
        "for doc in basic_search:\n",
        "  print(doc.page_content)\n",
        "\n",
        "# output\n",
        "# Jack loves to watch movies and listen to music.\n",
        "# Jack loves to trek on weekends.\n",
        "\n",
        "\n",
        "# contextual compress search\n",
        "base_retriever = vectorstore.as_retriever(\n",
        "    search_type=\"mmr\",\n",
        "    k = 2\n",
        ")\n",
        "base_retriever.invoke(QUERY1)\n",
        "base_retriever\n",
        "# output\n",
        "# [Document(metadata={}, page_content='Jack is friends with Tom and Sally.'),\n",
        "#  Document(metadata={}, page_content='Jack loves to watch movies and listen to music.'),\n",
        "#  Document(metadata={}, page_content='Jack loves to trek on weekends.'),\n",
        "#  Document(metadata={}, page_content='Jack likes to build workflows and AI systems')]\n",
        "\n",
        "similarityR = vectorstore.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    k = 2\n",
        ")\n",
        "print(\"x\"*12)\n",
        "res = similarityR.invoke(QUERY1)\n",
        "for doc in res:\n",
        "  print(doc.page_content)\n",
        "# basic_search\n",
        "\n",
        "# basic_search.invoke(QUERY1)\n",
        "\n",
        "# try sim search/ default, compress context search, MMR, compare results\n",
        "\n",
        "# compare perf, text splittint types for better perf?"
      ],
      "metadata": {
        "id": "LqoJCvkNjpsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Search Strategies"
      ],
      "metadata": {
        "id": "GgUznxEZMvrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# queries\n",
        "QUERY1 = \"who are Jack's friends?\"\n",
        "QUERY2 = \"what are jack's hobbies?\"\n",
        "QUERY3 = \"what does jack work on?\""
      ],
      "metadata": {
        "id": "juGSRhOINaNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic similarity search"
      ],
      "metadata": {
        "id": "_BpfyVxgMzDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# basic sim search\n",
        "basic_search = vectorstore.similarity_search(\n",
        "    query = QUERY2,\n",
        "    k = 2\n",
        ")\n",
        "for doc in basic_search:\n",
        "  print(doc.page_content)\n",
        "\n",
        "# Output\n",
        "# vectorstore.similary adhers to k; does not work invoking runnable (k)\n",
        "\n",
        "# Jack loves to watch movies and listen to music.\n",
        "# Jack loves to trek on weekends.\n"
      ],
      "metadata": {
        "id": "btqPneHGMt95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MMR"
      ],
      "metadata": {
        "id": "i3Y9yQqiNW6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MMR\n",
        "base_retriever = vectorstore.as_retriever(\n",
        "    search_type=\"mmr\",\n",
        "    search_kwargs = { \"k\": 2 }\n",
        ")\n",
        "res = base_retriever.invoke(QUERY2)\n",
        "for doc in res:\n",
        "  print(doc.page_content)\n",
        "\n",
        "# notes - k is ignored. why?; need to used search_kwargs, not \"k\"\n",
        "# output\n",
        "# Jack loves to watch movies and listen to music.\n",
        "# Jack likes to build workflows and AI systems\n"
      ],
      "metadata": {
        "id": "w71RXppSNJ2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## contextual compression thingie"
      ],
      "metadata": {
        "id": "tqqwBDNPQDJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# needs llm, embedding, compression mod, base retr, LLMChainExtractor\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain_openai import OpenAI, ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "model = ChatOpenAI(\n",
        "    model=\"gpt-3.5-turbo\")\n",
        "\n",
        "base_retriever = vectorstore.as_retriever(\n",
        "    search_type=\"mmr\",\n",
        "    search_kwargs = { \"k\": 2 }\n",
        ")\n",
        "\n",
        "base_compressor = LLMChainExtractor.from_llm(\n",
        "    llm = model,\n",
        ")\n",
        "\n",
        "compressor_retriever = ContextualCompressionRetriever(\n",
        "    base_retriever = base_retriever,\n",
        "    base_compressor = base_compressor\n",
        ")\n",
        "result_docs = compressor_retriever.invoke(QUERY3)\n",
        "\n",
        "# OUTPUTS - IMPRESSIVE!!\n",
        "\n",
        "# QUERY1\n",
        "# Jack is friends with Tom and Sally.\n",
        "\n",
        "# QUERY2\n",
        "# Jack loves to watch movies and listen to music.\n",
        "# Jack likes to build workflows and AI systems\n",
        "\n",
        "# QUERY3\n",
        "# Jack has worked on following technology stacks - Langchain, Langgraph, Langsmith\n",
        "\n",
        "for doc in result_docs:\n",
        "  print(doc.page_content)"
      ],
      "metadata": {
        "id": "iaCcG7lAQFNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quetions\n",
        "- choosing dimensions for embedding\n",
        "- chunking size\n",
        "- try sim search/ default, compress context search, MMR, compare results\n",
        "- compare perf, text splittint types for better perf?"
      ],
      "metadata": {
        "id": "U2AOeAZzUWn_"
      }
    }
  ]
}