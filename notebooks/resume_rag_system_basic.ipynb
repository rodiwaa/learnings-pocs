{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOaMcMptCPCrEMVjDpL6kAA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rodiwaa/learnings-pocs/blob/main/notebooks/resume_rag_system_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kkC9CEacOPOU"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain.openai openai langchain-community langsmith chromadb python-dotenv sentence-transformers pypdf langchain_community langchain_experimental"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## API keys from .env"
      ],
      "metadata": {
        "id": "ym20FDXoPOo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "load_dotenv(dotenv_path=\"/content/drive/MyDrive/Projects/.env/.env\")"
      ],
      "metadata": {
        "id": "ixIr6bmoPTmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read pdf from drive\n",
        "HOLD. will work with docs now, import PDF later."
      ],
      "metadata": {
        "id": "HcPXAkBARH4J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create docs for \"about me\""
      ],
      "metadata": {
        "id": "zax1ZAyOSHP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.vectorstores import Chroma\n",
        "# from langchain_openai import OpenAIEmbeddings\n",
        "# from sentence_transformers import SentenceTransformer # does not work well w langchain/chroma; use SentenceTransformerEmbeddings instead\n",
        "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
        "\n",
        "docs = [\n",
        "    Document(page_content=\"Jack likes to build workflows and AI systems\"),\n",
        "    Document(page_content=\"Jack has worked on following technology stacks - Langchain, Langgraph, Langsmith\"),\n",
        "    Document(page_content=\"Jack is friends with Tom and Sally.\"),\n",
        "    Document(page_content=\"Jack loves to trek on weekends.\"),\n",
        "    Document(page_content=\"Jack loves to watch movies and listen to music.\")\n",
        "]\n",
        "\n",
        "embedding_model = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "    embedding = embedding_model,\n",
        "    documents = docs,\n",
        "    collection_name = \"random_db_2\",\n",
        "    persist_directory = \"random_db_2\"\n",
        ")\n",
        "\n",
        "# check if docs are added\n",
        "added_docs = vectorstore.get()\n",
        "\n",
        "print(f\"added {len(added_docs)} docs\")\n",
        "print(added_docs)"
      ],
      "metadata": {
        "id": "3azxnay7RMQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrieve docs from VS based on query sim searches"
      ],
      "metadata": {
        "id": "zm_I96doji3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# delete chromadb to start from scratch\n",
        "# vectorstore.delete_collection()\n",
        "\n",
        "added = vectorstore.get()\n",
        "added"
      ],
      "metadata": {
        "id": "W8a_hI2OHyEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Search Strategies\n",
        "- similiarty\n",
        "- MMR\n",
        "- context compression\n",
        "- semantic chunker"
      ],
      "metadata": {
        "id": "GgUznxEZMvrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# queries\n",
        "QUERY1 = \"who are Jack's friends?\"\n",
        "QUERY2 = \"what are jack's hobbies?\"\n",
        "QUERY3 = \"what does jack work on?\""
      ],
      "metadata": {
        "id": "juGSRhOINaNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic similarity search"
      ],
      "metadata": {
        "id": "_BpfyVxgMzDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# basic sim search\n",
        "basic_search = vectorstore.similarity_search(\n",
        "    query = QUERY2,\n",
        "    k = 2\n",
        ")\n",
        "for doc in basic_search:\n",
        "  print(doc.page_content)\n",
        "\n",
        "# Output\n",
        "# vectorstore.similary adhers to k; does not work invoking runnable (k)\n",
        "\n",
        "# Jack loves to watch movies and listen to music.\n",
        "# Jack loves to trek on weekends.\n"
      ],
      "metadata": {
        "id": "btqPneHGMt95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MMR"
      ],
      "metadata": {
        "id": "i3Y9yQqiNW6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MMR\n",
        "base_retriever = vectorstore.as_retriever(\n",
        "    search_type=\"mmr\",\n",
        "    search_kwargs = { \"k\": 2 }\n",
        ")\n",
        "res = base_retriever.invoke(QUERY2)\n",
        "for doc in res:\n",
        "  print(doc.page_content)\n",
        "\n",
        "# notes - k is ignored. why?; need to used search_kwargs, not \"k\"\n",
        "# output\n",
        "# Jack loves to watch movies and listen to music.\n",
        "# Jack likes to build workflows and AI systems\n"
      ],
      "metadata": {
        "id": "w71RXppSNJ2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## contextual compression thingie"
      ],
      "metadata": {
        "id": "tqqwBDNPQDJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# needs llm, embedding, compression mod, base retr, LLMChainExtractor\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain_openai import OpenAI, ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "model = ChatOpenAI(\n",
        "    model=\"gpt-3.5-turbo\")\n",
        "\n",
        "base_retriever = vectorstore.as_retriever(\n",
        "    search_type=\"mmr\",\n",
        "    search_kwargs = { \"k\": 2 }\n",
        ")\n",
        "\n",
        "base_compressor = LLMChainExtractor.from_llm(\n",
        "    llm = model,\n",
        ")\n",
        "\n",
        "compressor_retriever = ContextualCompressionRetriever(\n",
        "    base_retriever = base_retriever,\n",
        "    base_compressor = base_compressor\n",
        ")\n",
        "result_docs = compressor_retriever.invoke(QUERY3)\n",
        "\n",
        "# OUTPUTS - IMPRESSIVE!!\n",
        "\n",
        "# QUERY1\n",
        "# Jack is friends with Tom and Sally.\n",
        "\n",
        "# QUERY2\n",
        "# Jack loves to watch movies and listen to music.\n",
        "# Jack likes to build workflows and AI systems\n",
        "\n",
        "# QUERY3\n",
        "# Jack has worked on following technology stacks - Langchain, Langgraph, Langsmith\n",
        "\n",
        "for doc in result_docs:\n",
        "  print(doc.page_content)"
      ],
      "metadata": {
        "id": "iaCcG7lAQFNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load pdf from drive"
      ],
      "metadata": {
        "id": "H02yFQbFfqg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "file_path = \"/content/drive/MyDrive/Projects/docs/rodi.pdf\"\n",
        "\n",
        "about_rodi_loader = PyPDFLoader(file_path)\n",
        "\n",
        "about_pdf_docs = about_rodi_loader.load()\n",
        "print(f\"docs loaded = {len(docs)}\")\n",
        "print(docs[0].page_content)\n",
        "doc = docs[0].page_content"
      ],
      "metadata": {
        "id": "Pt4Q4YWXfiss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple recursive text splitter"
      ],
      "metadata": {
        "id": "LFW82Kc8jqb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=10)\n",
        "\n",
        "chunks = splitter.split_text(doc)\n",
        "print(chunks)\n",
        "for doc in chunks:\n",
        "  print(doc)\n",
        "len(chunks)\n",
        "\n"
      ],
      "metadata": {
        "id": "wfWhaolii_uL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Semantic text splitter/ experiment\n",
        "to create semantic aware chunks"
      ],
      "metadata": {
        "id": "j_hS7rAxjv3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "llm = OpenAIEmbeddings(model='text-embedding-3-small')\n",
        "\n",
        "# about_loader is defined in another cell above. used Pydfloader to load PDF doc from gdrive.\n",
        "# about_loader = PyPDFLoader(file_path)\n",
        "\n",
        "about_rodi_docs = about_rodi_loader.load()\n",
        "print(\"dasdasdasd\")\n",
        "print(len(about_rodi_docs))\n",
        "print(about_rodi_docs)\n",
        "\n",
        "splitter = SemanticChunker(\n",
        "    embeddings = llm,\n",
        "    breakpoint_threshold_type=\"percentile\",\n",
        "    breakpoint_threshold_amount=90\n",
        ")\n",
        "\n",
        "docs1 = splitter.create_documents([about_pdf_docs[0].page_content])\n",
        "\n",
        "about_rodi_vs = vectorstore.from_documents(docs1, embedding=llm, collection_name=\"about_rodi1\")\n",
        "\n",
        "print(docs1)\n"
      ],
      "metadata": {
        "id": "dKVE6AbUktAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test queries\n",
        "QUERY1 = \"who is rohit?\"\n",
        "QUERY2 = \"what are rohit's hobbies?\"\n",
        "QUERY3 = \"what does rohit work on?\"\n",
        "QUERY4 = \"what projects has rohit worked on?\"\n",
        "\n",
        "\n",
        "# retriever = vectorstore.as_retriever(\n",
        "#     search_type=\"mmr\",\n",
        "#     search_kwargs = { \"k\": 2 }\n",
        "# )\n",
        "\n",
        "retriever = vectorstore.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs = { \"k\": 2 }\n",
        ")\n",
        "\n",
        "result_docs_q1 = retriever.invoke(QUERY1)\n",
        "result_docs_q2 = retriever.invoke(QUERY2)\n",
        "result_docs_q3 = retriever.invoke(QUERY3)\n",
        "result_docs_q4 = retriever.invoke(QUERY4)\n",
        "\n",
        "print(f\"q1\", {QUERY1})\n",
        "for doc in result_docs_q1:\n",
        "  print(doc.page_content)\n",
        "\n",
        "print(f\"\\nq2\", {QUERY2})\n",
        "for doc in result_docs_q2:\n",
        "  print(doc.page_content)\n",
        "\n",
        "print(f\"\\nq3\", {QUERY3})\n",
        "for doc in result_docs_q3:\n",
        "  print(doc.page_content)\n",
        "\n",
        "print(f\"\\nq3\", {QUERY4})\n",
        "for doc in result_docs_q4:\n",
        "  print(doc.page_content)\n",
        "\n",
        "\n",
        "# outputs\n",
        "\n",
        "# q1 {'who is rohit?'}\n",
        "# Jack likes to build workflows and AI systems\n",
        "# Jack has worked on following technology stacks - Langchain, Langgraph, Langsmith\n",
        "\n",
        "# q2 {\"what are rohit's hobbies?\"}\n",
        "# Jack loves to watch movies and listen to music.\n",
        "# Jack likes to build workflows and AI systems\n",
        "\n",
        "# q3 {'what does rohit work on?'}\n",
        "# Jack has worked on following technology stacks - Langchain, Langgraph, Langsmith\n",
        "# Jack likes to build workflows and AI systems\n",
        "\n",
        "# q3 {'what projects has rohit worked on?'}\n",
        "# Jack likes to build workflows and AI systems\n",
        "# Jack has worked on following technology stacks - Langchain, Langgraph, Langsmith"
      ],
      "metadata": {
        "id": "0DzBjOz7brrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zHqEUvkZk5tY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quetions\n",
        "- choosing dimensions for embedding\n",
        "- chunking size\n",
        "- try sim search/ default, compress context search, MMR, compare results\n",
        "- compare perf, text splittint types for better perf?"
      ],
      "metadata": {
        "id": "U2AOeAZzUWn_"
      }
    }
  ]
}