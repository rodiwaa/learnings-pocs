{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM6Ie1+G4x2zMMWHspK7a9m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rodiwaa/learnings-pocs/blob/main/notebooks/resume_rag_system_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kkC9CEacOPOU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "989ecb58-03aa-44ff-87d2-62f134815570"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Collecting langchain.openai\n",
            "  Downloading langchain_openai-0.3.34-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.30-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: langsmith in /usr/local/lib/python3.12/dist-packages (0.4.31)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-1.1.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.1)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-6.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain_experimental\n",
            "  Downloading langchain_experimental-0.3.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting qdrant-client\n",
            "  Downloading qdrant_client-1.15.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting langchain-qdrant\n",
            "  Downloading langchain_qdrant-0.2.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.77)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.9)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.3)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain.openai) (0.11.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Collecting requests<3,>=2 (from langchain)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.11.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith) (3.11.3)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langsmith) (25.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith) (0.25.0)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
            "Collecting pybase64>=1.4.1 (from chromadb)\n",
            "  Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.37.0)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.23.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.37.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.75.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.19.2)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-34.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.56.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.35.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Collecting portalocker<4.0,>=2.7.0 (from qdrant-client)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (5.29.5)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.12/dist-packages (from qdrant-client) (2.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.27.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Collecting urllib3<3,>=1.26.14 (from qdrant-client)\n",
            "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.58b0)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain.openai) (2024.11.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading langchain_openai-0.3.34-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.30-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chromadb-1.1.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-6.1.1-py3-none-any.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.5/323.5 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_experimental-0.3.4-py3-none-any.whl (209 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qdrant_client-1.15.1-py3-none-any.whl (337 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.3/337.3 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_qdrant-0.2.1-py3-none-any.whl (24 kB)\n",
            "Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading kubernetes-34.1.0-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.23.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.37.0-py3-none-any.whl (19 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Downloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (510 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.8/510.8 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=29225ddfbc3642dec6d1152a95e1f95eeae29566ac2cfd5c056cbe11109caff3\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/3d/69/8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, durationpy, uvloop, urllib3, pypdf, pybase64, portalocker, mypy-extensions, mmh3, marshmallow, humanfriendly, httptools, bcrypt, backoff, watchfiles, typing-inspect, requests, coloredlogs, posthog, onnxruntime, dataclasses-json, qdrant-client, kubernetes, opentelemetry-exporter-otlp-proto-grpc, langchain-qdrant, langchain.openai, chromadb, langchain-community, langchain_experimental\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.5.0\n",
            "    Uninstalling urllib3-2.5.0:\n",
            "      Successfully uninstalled urllib3-2.5.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 bcrypt-5.0.0 chromadb-1.1.0 coloredlogs-15.0.1 dataclasses-json-0.6.7 durationpy-0.10 httptools-0.6.4 humanfriendly-10.0 kubernetes-34.1.0 langchain-community-0.3.30 langchain-qdrant-0.2.1 langchain.openai-0.3.34 langchain_experimental-0.3.4 marshmallow-3.26.1 mmh3-5.2.0 mypy-extensions-1.1.0 onnxruntime-1.23.0 opentelemetry-exporter-otlp-proto-grpc-1.37.0 portalocker-3.2.0 posthog-5.4.0 pybase64-1.4.2 pypdf-6.1.1 pypika-0.48.9 qdrant-client-1.15.1 requests-2.32.5 typing-inspect-0.9.0 urllib3-2.3.0 uvloop-0.21.0 watchfiles-1.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain.openai \\\n",
        "  openai langchain-community langsmith chromadb \\\n",
        "  python-dotenv sentence-transformers pypdf \\\n",
        "  langchain_community langchain_experimental \\\n",
        "  qdrant-client langchain-qdrant qdrant-client"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## API keys from .env"
      ],
      "metadata": {
        "id": "ym20FDXoPOo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "load_dotenv(dotenv_path=\"/content/drive/MyDrive/Projects/.env/.env\")"
      ],
      "metadata": {
        "id": "ixIr6bmoPTmS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6980ad3d-3ba7-46aa-f96f-f642ac074aeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read and upload pdf from drive to Qdrant Cloud.\n",
        "HOLD. will work with docs now, import PDF later."
      ],
      "metadata": {
        "id": "HcPXAkBARH4J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get PDF content from gdrive"
      ],
      "metadata": {
        "id": "xZUtNKgsjoa2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pretty Print Docs Util"
      ],
      "metadata": {
        "id": "hBPbWYWV6w1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pretty_print_docs(docs):\n",
        "  for doc in docs:\n",
        "    clean_text = \" \".join(doc.page_content.splitlines()) # resolves multi line colab issue (\\n)\n",
        "    print(clean_text)"
      ],
      "metadata": {
        "id": "_qS15zQE6gPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "file_path = \"/content/drive/MyDrive/Projects/docs/rodi.pdf\"\n",
        "\n",
        "pdf_content = \"\"\"\n",
        "\n",
        "Rohit is a software engineer. For work, he automates complex business workflows\n",
        "and builds AI systems. He has been working for over 15 years in the tech industry.\n",
        "He has taken various roles, his latest role being of a Cloud Architect. He loves\n",
        "to watch drama movies and listens to music. He loves to take a drive and do\n",
        "road trips with friends and family, especially in the monsoons.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "about_rodi_loader = PyPDFLoader(file_path)\n",
        "\n",
        "about_pdf_docs = about_rodi_loader.load()\n",
        "print(f\"docs loaded = {len(about_pdf_docs)}\")\n",
        "# print(docs[0].page_content)\n",
        "\n",
        "for doc in about_pdf_docs:\n",
        "  print(doc.page_content)"
      ],
      "metadata": {
        "id": "xj3wQNXtj_B0",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# clean, split, chunk, embed and upload to vector store\n",
        "- lets use qdrant for public cloud URL"
      ],
      "metadata": {
        "id": "Gyz0fkHfjrxY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create docs for \"about me\""
      ],
      "metadata": {
        "id": "zax1ZAyOSHP_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrieve docs from VS based on query sim searches"
      ],
      "metadata": {
        "id": "zm_I96doji3r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Search Strategies\n",
        "- similiarty\n",
        "- MMR\n",
        "- context compression\n",
        "- semantic chunker"
      ],
      "metadata": {
        "id": "GgUznxEZMvrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# queries\n",
        "QUERY1 = \"who are rohit's friends?\"\n",
        "QUERY2 = \"what are rohit's hobbies?\"\n",
        "QUERY3 = \"what does rohit work on?\""
      ],
      "metadata": {
        "id": "juGSRhOINaNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic similarity search"
      ],
      "metadata": {
        "id": "_BpfyVxgMzDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# basic sim search\n",
        "basic_search = vectorstore.similarity_search(\n",
        "    query = QUERY2,\n",
        "    k = 2\n",
        ")\n",
        "for doc in basic_search:\n",
        "  print(doc.page_content)\n",
        "\n",
        "# Output\n",
        "# vectorstore.similary adhers to k; does not work invoking runnable (k)\n",
        "\n",
        "# Jack loves to watch movies and listen to music.\n",
        "# Jack loves to trek on weekends.\n"
      ],
      "metadata": {
        "id": "btqPneHGMt95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MMR"
      ],
      "metadata": {
        "id": "i3Y9yQqiNW6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MMR\n",
        "base_retriever = vectorstore.as_retriever(\n",
        "    search_type=\"mmr\",\n",
        "    search_kwargs = { \"k\": 2 }\n",
        ")\n",
        "res = base_retriever.invoke(QUERY2)\n",
        "for doc in res:\n",
        "  print(doc.page_content)\n",
        "\n",
        "# notes - k is ignored. why?; need to used search_kwargs, not \"k\"\n",
        "# output\n",
        "# Jack loves to watch movies and listen to music.\n",
        "# Jack likes to build workflows and AI systems\n"
      ],
      "metadata": {
        "id": "w71RXppSNJ2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup qdrant cloud vector store first"
      ],
      "metadata": {
        "id": "6ni3K325Uhwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-qdrant qdrant-client\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rI4f4yw5Ugw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_core.documents import Document\n",
        "from langchain_qdrant import Qdrant, QdrantVectorStore\n",
        "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
        "# from langchain_community.vectorstores import Chroma\n",
        "# from langchain_openai import OpenAIEmbeddings\n",
        "# from sentence_transformers import SentenceTransformer # does not work well w langchain/chroma; use SentenceTransformerEmbeddings instead\n",
        "\n",
        "# docs = [\n",
        "#     Document(page_content=\"Jack likes to build workflows and AI systems\"),\n",
        "#     Document(page_content=\"Jack has worked on following technology stacks - Langchain, Langgraph, Langsmith\"),\n",
        "#     Document(page_content=\"Jack is friends with Tom and Sally.\"),\n",
        "#     Document(page_content=\"Jack loves to trek on weekends.\"),\n",
        "#     Document(page_content=\"Jack loves to watch movies and listen to music.\")\n",
        "# ]\n",
        "\n",
        "print(f\"about_pdf_docs \\n {about_pdf_docs}\")\n",
        "\n",
        "docs = about_pdf_docs\n",
        "\n",
        "\n",
        "\n",
        "embedding_model = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# vectorstore = Chroma.from_documents(\n",
        "#     embedding = embedding_model,\n",
        "#     documents = docs,\n",
        "#     collection_name = \"random_db_2\",\n",
        "#     persist_directory = \"random_db_2\"\n",
        "# )\n",
        "\n",
        "qdrant_url = os.environ[\"QDRANT_URL\"]\n",
        "qdrant_api_key = os.environ[\"QDRANT_API_KEY\"]\n",
        "\n",
        "vectorstore = QdrantVectorStore.from_documents(\n",
        "    embedding = embedding_model,\n",
        "    documents = docs,\n",
        "    url=qdrant_url,\n",
        "    api_key=qdrant_api_key,\n",
        "    collection_name = \"about_rodi_rag\",\n",
        "    force_recreate = True\n",
        "    # persist_directory = \"about_rodi_rag\"\n",
        ")\n",
        "\n",
        "print(\"docs upload sucessfly to qdrant cloud collection about_rodi_rag\")\n",
        "\n",
        "# check if docs are added\n",
        "# added_docs = vectorstore.get()\n"
      ],
      "metadata": {
        "id": "3azxnay7RMQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## contextual compression thingie\n",
        "i will be using this one due to better results"
      ],
      "metadata": {
        "id": "tqqwBDNPQDJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# needs llm, embedding, compression mod, base retr, LLMChainExtractor\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain_openai import OpenAI, ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "model = ChatOpenAI(\n",
        "    model=\"gpt-3.5-turbo\")\n",
        "\n",
        "base_retriever = vectorstore.as_retriever(\n",
        "    search_type=\"mmr\",\n",
        "    search_kwargs = { \"k\": 2 }\n",
        ")\n",
        "\n",
        "base_compressor = LLMChainExtractor.from_llm(\n",
        "    llm = model,\n",
        ")\n",
        "\n",
        "compressor_retriever = ContextualCompressionRetriever(\n",
        "    base_retriever = base_retriever,\n",
        "    base_compressor = base_compressor\n",
        ")\n",
        "result_docs = compressor_retriever.invoke(QUERY3)\n",
        "\n",
        "# OUTPUTS - IMPRESSIVE!!\n",
        "\n",
        "# QUERY1\n",
        "# Jack is friends with Tom and Sally.\n",
        "\n",
        "# QUERY2\n",
        "# Jack loves to watch movies and listen to music.\n",
        "# Jack likes to build workflows and AI systems\n",
        "\n",
        "# QUERY3\n",
        "# Jack has worked on following technology stacks - Langchain, Langgraph, Langsmith\n",
        "\n",
        "for doc in result_docs:\n",
        "  print(doc.page_content)"
      ],
      "metadata": {
        "id": "iaCcG7lAQFNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### more testing needed here for invokes"
      ],
      "metadata": {
        "id": "Twyozol_Zj5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### test invoke with all 3 queries\n",
        "# print(\"\\n\")\n",
        "# print(QUERY1)\n",
        "# result_docs1 = compressor_retriever.invoke(QUERY1)\n",
        "# result_docs1[0].page_content\n",
        "\n",
        "# print(\"\\n\")\n",
        "# print(QUERY2)\n",
        "# result_docs2 = compressor_retriever.invoke(QUERY2)\n",
        "# result_docs2[0].page_content\n",
        "\n",
        "# print(\"\\n\")\n",
        "# print(QUERY3)\n",
        "# result_docs3 = compressor_retriever.invoke(QUERY3)\n",
        "# result_docs3[0].page_content\n",
        "\n",
        "QUERY4 = \"what is rohit's tech stack?\"\n",
        "print(QUERY4)\n",
        "result_docs4 = compressor_retriever.invoke(QUERY4)\n",
        "# result_docs4[0].page_content\n",
        "\n",
        "print(result_docs4)"
      ],
      "metadata": {
        "id": "BcgynONSXpFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup qdrant cloud\n",
        "\n",
        "### init qdrant cloud and save chunks to VS"
      ],
      "metadata": {
        "id": "iLhF0j0AQpEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(os.environ[\"QDRANT_API_KEY\"])\n",
        "print(os.environ[\"QDRANT_URL\"])\n"
      ],
      "metadata": {
        "id": "Av50G7fZQn13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load pdf from drive"
      ],
      "metadata": {
        "id": "H02yFQbFfqg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "file_path = \"/content/drive/MyDrive/Projects/docs/rodi.pdf\"\n",
        "\n",
        "about_rodi_loader = PyPDFLoader(file_path)\n",
        "\n",
        "about_pdf_docs = about_rodi_loader.load()\n",
        "print(f\"docs loaded = {len(docs)}\")\n",
        "print(docs[0].page_content)\n",
        "doc = docs[0].page_content"
      ],
      "metadata": {
        "id": "Pt4Q4YWXfiss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple recursive text splitter"
      ],
      "metadata": {
        "id": "LFW82Kc8jqb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=10)\n",
        "\n",
        "chunks = splitter.split_text(doc)\n",
        "print(chunks)\n",
        "for doc in chunks:\n",
        "  print(doc)\n",
        "len(chunks)\n",
        "\n"
      ],
      "metadata": {
        "id": "wfWhaolii_uL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Semantic text splitter/ experiment\n",
        "to create semantic aware chunks"
      ],
      "metadata": {
        "id": "j_hS7rAxjv3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "llm = OpenAIEmbeddings(model='text-embedding-3-small')\n",
        "\n",
        "# about_loader is defined in another cell above. used Pydfloader to load PDF doc from gdrive.\n",
        "# about_loader = PyPDFLoader(file_path)\n",
        "\n",
        "about_rodi_docs = about_rodi_loader.load()\n",
        "print(\"dasdasdasd\")\n",
        "print(len(about_rodi_docs))\n",
        "print(about_rodi_docs)\n",
        "\n",
        "splitter = SemanticChunker(\n",
        "    embeddings = llm,\n",
        "    breakpoint_threshold_type=\"percentile\",\n",
        "    breakpoint_threshold_amount=90\n",
        ")\n",
        "\n",
        "docs1 = splitter.create_documents([about_pdf_docs[0].page_content])\n",
        "\n",
        "about_rodi_vs = vectorstore.from_documents(docs1, embedding=llm, collection_name=\"about_rodi1\")\n",
        "\n",
        "print(docs1)\n"
      ],
      "metadata": {
        "id": "dKVE6AbUktAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup qdrant cloud retriever (OG)"
      ],
      "metadata": {
        "id": "hVb99Rgdn22Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test queries\n",
        "QUERY1 = \"who is rohit?\"\n",
        "QUERY2 = \"what are rohit's hobbies?\"\n",
        "QUERY3 = \"what does rohit work on?\"\n",
        "QUERY4 = \"what projects has rohit worked on?\"\n",
        "\n",
        "\n",
        "# retriever = vectorstore.as_retriever(\n",
        "#     search_type=\"mmr\",\n",
        "#     search_kwargs = { \"k\": 2 }\n",
        "# )\n",
        "\n",
        "retriever = vectorstore.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs = { \"k\": 2 }\n",
        ")\n",
        "\n",
        "result_docs_q1 = retriever.invoke(QUERY1)\n",
        "result_docs_q2 = retriever.invoke(QUERY2)\n",
        "result_docs_q3 = retriever.invoke(QUERY3)\n",
        "result_docs_q4 = retriever.invoke(QUERY4)\n",
        "\n",
        "print(f\"q1\", {QUERY1})\n",
        "for doc in result_docs_q1:\n",
        "  print(doc.page_content)\n",
        "\n",
        "print(f\"\\nq2\", {QUERY2})\n",
        "for doc in result_docs_q2:\n",
        "  print(doc.page_content)\n",
        "\n",
        "print(f\"\\nq3\", {QUERY3})\n",
        "for doc in result_docs_q3:\n",
        "  print(doc.page_content)\n",
        "\n",
        "print(f\"\\nq3\", {QUERY4})\n",
        "for doc in result_docs_q4:\n",
        "  print(doc.page_content)\n",
        "\n",
        "\n",
        "# outputs\n",
        "\n",
        "# q1 {'who is rohit?'}\n",
        "# Jack likes to build workflows and AI systems\n",
        "# Jack has worked on following technology stacks - Langchain, Langgraph, Langsmith\n",
        "\n",
        "# q2 {\"what are rohit's hobbies?\"}\n",
        "# Jack loves to watch movies and listen to music.\n",
        "# Jack likes to build workflows and AI systems\n",
        "\n",
        "# q3 {'what does rohit work on?'}\n",
        "# Jack has worked on following technology stacks - Langchain, Langgraph, Langsmith\n",
        "# Jack likes to build workflows and AI systems\n",
        "\n",
        "# q3 {'what projects has rohit worked on?'}\n",
        "# Jack likes to build workflows and AI systems\n",
        "# Jack has worked on following technology stacks - Langchain, Langgraph, Langsmith"
      ],
      "metadata": {
        "id": "0DzBjOz7brrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup qdrant retriever TEST"
      ],
      "metadata": {
        "id": "YeEdJrLwtQ-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_qdrant import Qdrant, QdrantVectorStore\n",
        "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
        "\n",
        "# test queries\n",
        "QUERY1 = \"who is rohit?\"\n",
        "QUERY2 = \"what are rohit's hobbies?\"\n",
        "QUERY3 = \"what does rohit like to do?\"\n",
        "QUERY4 = \"what is rohit's favourite drink?\"\n",
        "\n",
        "\n",
        "# retriever = vectorstore.as_retriever(\n",
        "#     search_type=\"mmr\",\n",
        "#     search_kwargs = { \"k\": 2 }\n",
        "# )\n",
        "\n",
        "# create new vectorstore name to separate from OG above\n",
        "\n",
        "llm_model = \"all-MiniLM-L6-v2\"\n",
        "embedding_model = SentenceTransformerEmbeddings(model_name=llm_model)\n",
        "\n",
        "retriever = QdrantVectorStore.from_existing_collection(\n",
        "    embedding = embedding_model,\n",
        "    # documents = docs,\n",
        "    url=os.environ[\"QDRANT_URL\"],\n",
        "    api_key=os.environ[\"QDRANT_API_KEY\"],\n",
        "    # collection_name = \"about_rodi_rag\",\n",
        "    collection_name=\"test_coll_name\" # new rag testing collection\n",
        "\n",
        ").as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs = { \"k\": 2 }\n",
        ")\n",
        "\n",
        "result_docs_q1 = retriever.invoke(QUERY1)\n",
        "result_docs_q2 = retriever.invoke(QUERY2)\n",
        "result_docs_q3 = retriever.invoke(QUERY3)\n",
        "result_docs_q4 = retriever.invoke(QUERY4)\n",
        "\n",
        "print(f\"q1\", {QUERY1})\n",
        "for doc in result_docs_q1:\n",
        "  print(\" \".join(doc.page_content.splitlines()))\n",
        "\n",
        "print(f\"\\nq2\", {QUERY2})\n",
        "for doc in result_docs_q2:\n",
        "  print(\" \".join(doc.page_content.splitlines()))\n",
        "\n",
        "print(f\"\\nq3\", {QUERY3})\n",
        "for doc in result_docs_q3:\n",
        "  print(\" \".join(doc.page_content.splitlines()))\n",
        "\n",
        "print(f\"\\nq3\", {QUERY4})\n",
        "for doc in result_docs_q4:\n",
        "  print(\" \".join(doc.page_content.splitlines()))\n",
        "\n",
        "\n",
        "# outputs\n",
        "\n",
        "# q1 {'who is rohit?'}\n",
        "# Jack likes to build workflows and AI systems\n",
        "# Jack has worked on following technology stacks - Langchain, Langgraph, Langsmith\n",
        "\n",
        "# q2 {\"what are rohit's hobbies?\"}\n",
        "# Jack loves to watch movies and listen to music.\n",
        "# Jack likes to build workflows and AI systems\n",
        "\n",
        "# q3 {'what does rohit work on?'}\n",
        "# Jack has worked on following technology stacks - Langchain, Langgraph, Langsmith\n",
        "# Jack likes to build workflows and AI systems\n",
        "\n",
        "# q3 {'what projects has rohit worked on?'}\n",
        "# Jack likes to build workflows and AI systems\n",
        "# Jack has worked on following technology stacks - Langchain, Langgraph, Langsmith"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbyVcMjwtMYl",
        "outputId": "0b0f515b-8cf2-4318-c435-385c6fc22f12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "q1 {'who is rohit?'}\n",
            "Rohit likes to drink beer\n",
            "Rohit likes to drink coffee\n",
            "\n",
            "q2 {\"what are rohit's hobbies?\"}\n",
            "Rohit likes to drink beer\n",
            "Rohit likes to drink coffee\n",
            "\n",
            "q3 {'what does rohit like to do?'}\n",
            "Rohit likes to drink beer\n",
            "Rohit likes to drink coffee\n",
            "\n",
            "q3 {\"what is rohit's favourite drink?\"}\n",
            "Rohit likes to drink beer\n",
            "Rohit likes to drink coffee\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup qdrant TEST 2"
      ],
      "metadata": {
        "id": "34H9x2G715Wp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "from langchain_qdrant import Qdrant, QdrantVectorStore\n",
        "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain_openai import OpenAI, ChatOpenAI\n",
        "\n",
        "QUERY1 = \"what does rohit eat?\"\n",
        "QUERY2 = \"what are rohit's hobbies?\"\n",
        "QUERY3 = \"what does rohit drink?\"\n",
        "\n",
        "\n",
        "def create_vector_store_retriever():\n",
        "  # llm_model = ChatOpenAI(\n",
        "  #     model=\"gpt-3.5-turbo\")\n",
        "\n",
        "  embedding_model = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "\n",
        "  # base_retriever = QdrantVectorStore.from_existing_collection(\n",
        "  #   embedding = embedding_model,\n",
        "  #   # documents = docs,\n",
        "  #   url=os.environ[\"QDRANT_URL\"],\n",
        "  #   api_key=os.environ[\"QDRANT_API_KEY\"],\n",
        "  #   collection_name = \"test_coll_name\",\n",
        "  # ).as_retriever(\n",
        "  #     search_type=\"mmr\",\n",
        "  #     search_kwargs = { \"k\": 2 }\n",
        "  # )\n",
        "\n",
        "  base_retriever = QdrantVectorStore.from_existing_collection(\n",
        "    embedding = embedding_model,\n",
        "    # documents = docs,\n",
        "    url=os.environ[\"QDRANT_URL\"],\n",
        "    api_key=os.environ[\"QDRANT_API_KEY\"],\n",
        "    collection_name = \"test_coll_name\",\n",
        "  ).similarity_search(\n",
        "      # search_type=\"mmr\",\n",
        "      # search_kwargs = { \"k\": 2 }\n",
        "      query=QUERY3,\n",
        "      k=2\n",
        "  )\n",
        "\n",
        "  pretty_print_docs(base_retriever)\n",
        "\n",
        "  # print(\"base_retriever\")\n",
        "  # print(base_retriever)\n",
        "\n",
        "  # base_compressor = LLMChainExtractor.from_llm(\n",
        "  #     llm = llm_model, #needs to be llm_model, not embedding model here.\n",
        "  # )\n",
        "  # print(\"base_compressor\")\n",
        "\n",
        "  # compressor_retriever = ContextualCompressionRetriever(\n",
        "  #     base_retriever = base_retriever,\n",
        "  #     base_compressor = base_compressor\n",
        "  # )\n",
        "  # result_docs = compressor_retriever.invoke(QUERY3)\n",
        "  # result_docs = base_retriever.invoke(QUERY3)\n",
        "\n",
        "  # print(f\"\\nQUERY1 - {QUERY1}\\n\")\n",
        "  # print(f\"\\nQUERY2 - {QUERY2}\\n\")\n",
        "  # print(f\"\\nQUERY3 - {QUERY3}\\n\")\n",
        "  # for doc in result_docs:\n",
        "  #   print(doc.page_content)\n",
        "  # return result_docs\n",
        "\n",
        "print('calling...')\n",
        "create_vector_store_retriever()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYbBUEYl14K7",
        "outputId": "b6dc70ab-241a-407d-f695-1be29ef6bc39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "calling...\n",
            "Rohit likes to drink beer\n",
            "Rohit likes to drink coffee\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Next steps -\n",
        "host n8n on aws ec2\n",
        "- run cron workflows to test availability\n",
        "- add to website\n",
        "- test latency after obervability/ langSmith\n",
        "\n",
        "- setup chainlit for chat UI\n",
        "- modularise cells in nodes-blocks and langgraphy the whole thing\n",
        "- setup fastapi/ cl handlers to interact\n",
        "- observability\n",
        "  - traces\n",
        "  - latency\n",
        "  - bottlenecks"
      ],
      "metadata": {
        "id": "uDkmmPv9ZrFq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimisations - Future\n",
        "- iterate and improve sematnic chunking and retrieval\n",
        "- multi modal? nope.\n",
        "- audio input - via chainlit cl.on_audio_chunk"
      ],
      "metadata": {
        "id": "k8PiehxKaH3E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quetions\n",
        "- choosing dimensions for embedding\n",
        "- chunking size\n",
        "- try sim search/ default, compress context search, MMR, compare results\n",
        "- compare perf, text splittint types for better perf?"
      ],
      "metadata": {
        "id": "U2AOeAZzUWn_"
      }
    }
  ]
}