{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOv05cQEwKmc2DM9d5BkC9q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rodiwaa/learnings-pocs/blob/main/notebooks/resume_rag_system_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kkC9CEacOPOU"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain.openai openai langchain-community langsmith chromadb python-dotenv sentence-transformers pypdf langchain_community langchain_experimental qdrant-client"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## API keys from .env"
      ],
      "metadata": {
        "id": "ym20FDXoPOo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "load_dotenv(dotenv_path=\"/content/drive/MyDrive/Projects/.env/.env\")"
      ],
      "metadata": {
        "id": "ixIr6bmoPTmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read and upload pdf from drive to Qdrant Cloud.\n",
        "HOLD. will work with docs now, import PDF later."
      ],
      "metadata": {
        "id": "HcPXAkBARH4J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get PDF content from gdrive"
      ],
      "metadata": {
        "id": "xZUtNKgsjoa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "file_path = \"/content/drive/MyDrive/Projects/docs/rodi.pdf\"\n",
        "\n",
        "pdf_content = \"\"\"\n",
        "\n",
        "Rohit is a software engineer. For work, he automates complex business workflows\n",
        "and builds AI systems. He has been working for over 15 years in the tech industry.\n",
        "He has taken various roles, his latest role being of a Cloud Architect. He loves\n",
        "to watch drama movies and listens to music. He loves to take a drive and do\n",
        "road trips with friends and family, especially in the monsoons.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "about_rodi_loader = PyPDFLoader(file_path)\n",
        "\n",
        "about_pdf_docs = about_rodi_loader.load()\n",
        "print(f\"docs loaded = {len(about_pdf_docs)}\")\n",
        "# print(docs[0].page_content)\n",
        "\n",
        "for doc in about_pdf_docs:\n",
        "  print(doc.page_content)"
      ],
      "metadata": {
        "id": "xj3wQNXtj_B0",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# clean, split, chunk, embed and upload to vector store\n",
        "- lets use qdrant for public cloud URL"
      ],
      "metadata": {
        "id": "Gyz0fkHfjrxY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create docs for \"about me\""
      ],
      "metadata": {
        "id": "zax1ZAyOSHP_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrieve docs from VS based on query sim searches"
      ],
      "metadata": {
        "id": "zm_I96doji3r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Search Strategies\n",
        "- similiarty\n",
        "- MMR\n",
        "- context compression\n",
        "- semantic chunker"
      ],
      "metadata": {
        "id": "GgUznxEZMvrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# queries\n",
        "QUERY1 = \"who are rohit's friends?\"\n",
        "QUERY2 = \"what are rohit's hobbies?\"\n",
        "QUERY3 = \"what does rohit work on?\""
      ],
      "metadata": {
        "id": "juGSRhOINaNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic similarity search"
      ],
      "metadata": {
        "id": "_BpfyVxgMzDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# basic sim search\n",
        "basic_search = vectorstore.similarity_search(\n",
        "    query = QUERY2,\n",
        "    k = 2\n",
        ")\n",
        "for doc in basic_search:\n",
        "  print(doc.page_content)\n",
        "\n",
        "# Output\n",
        "# vectorstore.similary adhers to k; does not work invoking runnable (k)\n",
        "\n",
        "# Jack loves to watch movies and listen to music.\n",
        "# Jack loves to trek on weekends.\n"
      ],
      "metadata": {
        "id": "btqPneHGMt95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MMR"
      ],
      "metadata": {
        "id": "i3Y9yQqiNW6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MMR\n",
        "base_retriever = vectorstore.as_retriever(\n",
        "    search_type=\"mmr\",\n",
        "    search_kwargs = { \"k\": 2 }\n",
        ")\n",
        "res = base_retriever.invoke(QUERY2)\n",
        "for doc in res:\n",
        "  print(doc.page_content)\n",
        "\n",
        "# notes - k is ignored. why?; need to used search_kwargs, not \"k\"\n",
        "# output\n",
        "# Jack loves to watch movies and listen to music.\n",
        "# Jack likes to build workflows and AI systems\n"
      ],
      "metadata": {
        "id": "w71RXppSNJ2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup qdrant cloud vector store first"
      ],
      "metadata": {
        "id": "6ni3K325Uhwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-qdrant qdrant-client\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rI4f4yw5Ugw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_core.documents import Document\n",
        "from langchain_qdrant import Qdrant, QdrantVectorStore\n",
        "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
        "# from langchain_community.vectorstores import Chroma\n",
        "# from langchain_openai import OpenAIEmbeddings\n",
        "# from sentence_transformers import SentenceTransformer # does not work well w langchain/chroma; use SentenceTransformerEmbeddings instead\n",
        "\n",
        "# docs = [\n",
        "#     Document(page_content=\"Jack likes to build workflows and AI systems\"),\n",
        "#     Document(page_content=\"Jack has worked on following technology stacks - Langchain, Langgraph, Langsmith\"),\n",
        "#     Document(page_content=\"Jack is friends with Tom and Sally.\"),\n",
        "#     Document(page_content=\"Jack loves to trek on weekends.\"),\n",
        "#     Document(page_content=\"Jack loves to watch movies and listen to music.\")\n",
        "# ]\n",
        "\n",
        "print(about_pdf_docs)\n",
        "\n",
        "docs = about_pdf_docs\n",
        "\n",
        "\n",
        "\n",
        "embedding_model = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# vectorstore = Chroma.from_documents(\n",
        "#     embedding = embedding_model,\n",
        "#     documents = docs,\n",
        "#     collection_name = \"random_db_2\",\n",
        "#     persist_directory = \"random_db_2\"\n",
        "# )\n",
        "\n",
        "qdrant_url = os.environ[\"QDRANT_URL\"]\n",
        "qdrant_api_key = os.environ[\"QDRANT_API_KEY\"]\n",
        "\n",
        "vectorstore = QdrantVectorStore.from_documents(\n",
        "    embedding = embedding_model,\n",
        "    documents = docs,\n",
        "    url=qdrant_url,\n",
        "    api_key=qdrant_api_key,\n",
        "    collection_name = \"about_rodi_rag\",\n",
        "    force_recreate = True\n",
        "    # persist_directory = \"about_rodi_rag\"\n",
        ")\n",
        "\n",
        "print(\"docs upload sucessfly to qdrant cloud collection about_rodi_rag\")\n",
        "\n",
        "# check if docs are added\n",
        "# added_docs = vectorstore.get()\n"
      ],
      "metadata": {
        "id": "3azxnay7RMQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## contextual compression thingie\n",
        "i will be using this one due to better results"
      ],
      "metadata": {
        "id": "tqqwBDNPQDJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# needs llm, embedding, compression mod, base retr, LLMChainExtractor\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain_openai import OpenAI, ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "model = ChatOpenAI(\n",
        "    model=\"gpt-3.5-turbo\")\n",
        "\n",
        "base_retriever = vectorstore.as_retriever(\n",
        "    search_type=\"mmr\",\n",
        "    search_kwargs = { \"k\": 2 }\n",
        ")\n",
        "\n",
        "base_compressor = LLMChainExtractor.from_llm(\n",
        "    llm = model,\n",
        ")\n",
        "\n",
        "compressor_retriever = ContextualCompressionRetriever(\n",
        "    base_retriever = base_retriever,\n",
        "    base_compressor = base_compressor\n",
        ")\n",
        "result_docs = compressor_retriever.invoke(QUERY3)\n",
        "\n",
        "# OUTPUTS - IMPRESSIVE!!\n",
        "\n",
        "# QUERY1\n",
        "# Jack is friends with Tom and Sally.\n",
        "\n",
        "# QUERY2\n",
        "# Jack loves to watch movies and listen to music.\n",
        "# Jack likes to build workflows and AI systems\n",
        "\n",
        "# QUERY3\n",
        "# Jack has worked on following technology stacks - Langchain, Langgraph, Langsmith\n",
        "\n",
        "for doc in result_docs:\n",
        "  print(doc.page_content)"
      ],
      "metadata": {
        "id": "iaCcG7lAQFNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### more testing needed here for invokes"
      ],
      "metadata": {
        "id": "Twyozol_Zj5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### test invoke with all 3 queries\n",
        "# print(\"\\n\")\n",
        "# print(QUERY1)\n",
        "# result_docs1 = compressor_retriever.invoke(QUERY1)\n",
        "# result_docs1[0].page_content\n",
        "\n",
        "# print(\"\\n\")\n",
        "# print(QUERY2)\n",
        "# result_docs2 = compressor_retriever.invoke(QUERY2)\n",
        "# result_docs2[0].page_content\n",
        "\n",
        "# print(\"\\n\")\n",
        "# print(QUERY3)\n",
        "# result_docs3 = compressor_retriever.invoke(QUERY3)\n",
        "# result_docs3[0].page_content\n",
        "\n",
        "QUERY4 = \"what is rohit's tech stack?\"\n",
        "print(QUERY4)\n",
        "result_docs4 = compressor_retriever.invoke(QUERY4)\n",
        "# result_docs4[0].page_content\n",
        "\n",
        "print(result_docs4)"
      ],
      "metadata": {
        "id": "BcgynONSXpFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup qdrant cloud\n",
        "\n",
        "### init qdrant cloud and save chunks to VS"
      ],
      "metadata": {
        "id": "iLhF0j0AQpEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(os.environ[\"QDRANT_API_KEY\"])\n",
        "print(os.environ[\"QDRANT_URL\"])\n"
      ],
      "metadata": {
        "id": "Av50G7fZQn13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load pdf from drive"
      ],
      "metadata": {
        "id": "H02yFQbFfqg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "file_path = \"/content/drive/MyDrive/Projects/docs/rodi.pdf\"\n",
        "\n",
        "about_rodi_loader = PyPDFLoader(file_path)\n",
        "\n",
        "about_pdf_docs = about_rodi_loader.load()\n",
        "print(f\"docs loaded = {len(docs)}\")\n",
        "print(docs[0].page_content)\n",
        "doc = docs[0].page_content"
      ],
      "metadata": {
        "id": "Pt4Q4YWXfiss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple recursive text splitter"
      ],
      "metadata": {
        "id": "LFW82Kc8jqb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=10)\n",
        "\n",
        "chunks = splitter.split_text(doc)\n",
        "print(chunks)\n",
        "for doc in chunks:\n",
        "  print(doc)\n",
        "len(chunks)\n",
        "\n"
      ],
      "metadata": {
        "id": "wfWhaolii_uL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Semantic text splitter/ experiment\n",
        "to create semantic aware chunks"
      ],
      "metadata": {
        "id": "j_hS7rAxjv3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "llm = OpenAIEmbeddings(model='text-embedding-3-small')\n",
        "\n",
        "# about_loader is defined in another cell above. used Pydfloader to load PDF doc from gdrive.\n",
        "# about_loader = PyPDFLoader(file_path)\n",
        "\n",
        "about_rodi_docs = about_rodi_loader.load()\n",
        "print(\"dasdasdasd\")\n",
        "print(len(about_rodi_docs))\n",
        "print(about_rodi_docs)\n",
        "\n",
        "splitter = SemanticChunker(\n",
        "    embeddings = llm,\n",
        "    breakpoint_threshold_type=\"percentile\",\n",
        "    breakpoint_threshold_amount=90\n",
        ")\n",
        "\n",
        "docs1 = splitter.create_documents([about_pdf_docs[0].page_content])\n",
        "\n",
        "about_rodi_vs = vectorstore.from_documents(docs1, embedding=llm, collection_name=\"about_rodi1\")\n",
        "\n",
        "print(docs1)\n"
      ],
      "metadata": {
        "id": "dKVE6AbUktAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test queries\n",
        "QUERY1 = \"who is rohit?\"\n",
        "QUERY2 = \"what are rohit's hobbies?\"\n",
        "QUERY3 = \"what does rohit work on?\"\n",
        "QUERY4 = \"what projects has rohit worked on?\"\n",
        "\n",
        "\n",
        "# retriever = vectorstore.as_retriever(\n",
        "#     search_type=\"mmr\",\n",
        "#     search_kwargs = { \"k\": 2 }\n",
        "# )\n",
        "\n",
        "retriever = vectorstore.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs = { \"k\": 2 }\n",
        ")\n",
        "\n",
        "result_docs_q1 = retriever.invoke(QUERY1)\n",
        "result_docs_q2 = retriever.invoke(QUERY2)\n",
        "result_docs_q3 = retriever.invoke(QUERY3)\n",
        "result_docs_q4 = retriever.invoke(QUERY4)\n",
        "\n",
        "print(f\"q1\", {QUERY1})\n",
        "for doc in result_docs_q1:\n",
        "  print(doc.page_content)\n",
        "\n",
        "print(f\"\\nq2\", {QUERY2})\n",
        "for doc in result_docs_q2:\n",
        "  print(doc.page_content)\n",
        "\n",
        "print(f\"\\nq3\", {QUERY3})\n",
        "for doc in result_docs_q3:\n",
        "  print(doc.page_content)\n",
        "\n",
        "print(f\"\\nq3\", {QUERY4})\n",
        "for doc in result_docs_q4:\n",
        "  print(doc.page_content)\n",
        "\n",
        "\n",
        "# outputs\n",
        "\n",
        "# q1 {'who is rohit?'}\n",
        "# Jack likes to build workflows and AI systems\n",
        "# Jack has worked on following technology stacks - Langchain, Langgraph, Langsmith\n",
        "\n",
        "# q2 {\"what are rohit's hobbies?\"}\n",
        "# Jack loves to watch movies and listen to music.\n",
        "# Jack likes to build workflows and AI systems\n",
        "\n",
        "# q3 {'what does rohit work on?'}\n",
        "# Jack has worked on following technology stacks - Langchain, Langgraph, Langsmith\n",
        "# Jack likes to build workflows and AI systems\n",
        "\n",
        "# q3 {'what projects has rohit worked on?'}\n",
        "# Jack likes to build workflows and AI systems\n",
        "# Jack has worked on following technology stacks - Langchain, Langgraph, Langsmith"
      ],
      "metadata": {
        "id": "0DzBjOz7brrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Next steps -\n",
        "- setup chainlit for chat UI\n",
        "- modularise cells in nodes-blocks and langgraphy the whole thing\n",
        "- setup fastapi/ cl handlers to interact\n",
        "- observability\n",
        "  - traces\n",
        "  - latency\n",
        "  - bottlenecks"
      ],
      "metadata": {
        "id": "uDkmmPv9ZrFq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimisations - Future\n",
        "- iterate and improve sematnic chunking and retrieval\n",
        "- multi modal? nope.\n",
        "- audio input - via chainlit cl.on_audio_chunk"
      ],
      "metadata": {
        "id": "k8PiehxKaH3E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quetions\n",
        "- choosing dimensions for embedding\n",
        "- chunking size\n",
        "- try sim search/ default, compress context search, MMR, compare results\n",
        "- compare perf, text splittint types for better perf?"
      ],
      "metadata": {
        "id": "U2AOeAZzUWn_"
      }
    }
  ]
}